{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=0\n",
    "csd_mt = csd_multitaper(GA_epoches[s], fmin=1, fmax=3, adaptive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "c = Client('10.96.2.137:8786')# Listen on TCP port 8786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  dask_ml   # register the distriubted backend\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, random_state=0)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "              \"kernel\": ['rbf', 'poly', 'sigmoid'],\n",
    "              \"shrinking\": [True, False]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(gamma='auto', random_state=0, probability=True),\n",
    "                           param_grid=param_grid,\n",
    "                           return_train_score=False,\n",
    "                           iid=True,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.externals  import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask', scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Worker\n",
    "w = Worker(scheduler='tcp://10.96.8.3:63816')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore some frequency bands\n",
    "iter_freqs = [\n",
    "    ('fr', 1, 3),\n",
    "    ('fr', 4, 6)\n",
    "]\n",
    "\n",
    "\n",
    "raw_fname = data_path + '/data/SurrogateCoherence/SurrogateCoherence-'+ str(remove_first)+'.mat'\n",
    "mat = scipy.io.loadmat(raw_fname)\n",
    "\n",
    "Feature= mat['data']['Feature'][0]\n",
    "Condition= mat['data']['Condition'][0]\n",
    "Delay= mat['data']['Delay'][0]\n",
    "noTrials= mat['data']['noTrials'][0]\n",
    "Subject= mat['data']['Subject'][0]\n",
    "originalCoh= mat['data']['Data'][0]\n",
    "surrogateCoh= mat['data']['Surrogate'][0]\n",
    "\n",
    "Feature=np.concatenate( Feature[0][0,:], axis=0 )\n",
    "Condition=np.concatenate( Condition[0][0,:], axis=0 )\n",
    "Delay=np.concatenate( Delay[0][0,:], axis=0 )\n",
    "noTrials=np.concatenate( noTrials[0][0,:], axis=0 )\n",
    "Subject=np.concatenate( Subject[0][0,:], axis=0 )\n",
    "    \n",
    "\n",
    "frames = []\n",
    "for i in range(0,len(Subject)):\n",
    "    Ocoh = originalCoh[0][0,:][i]\n",
    "    Scoh = surrogateCoh[0][0,:][i]\n",
    "\n",
    "    for band, fmin, fmax in iter_freqs:\n",
    "        a = band+ ' '+str(fmin)+' - '+str(fmax)+'Hz' \n",
    "        x = Ocoh[:,fmin:fmax].mean(axis=1)\n",
    "        y = Scoh[:,fmin:fmax].mean(axis=1)\n",
    "\n",
    "        df = pd.DataFrame({'Feature':Feature[i],'Condition':Condition[i],'Delay':Delay[i],\n",
    "                     'noTrials':noTrials[:,0][i],'Subject':Subject[i],'Freq':a,'Data':[x],'Surrogate':[y]})\n",
    "        frames.append(df)\n",
    "        \n",
    "data_surrogate=pd.concat((frames),axis=0)\n",
    "data_surrogate['Delay'] = data_surrogate['Delay'].astype('float64')\n",
    "data_surrogate['Delay']=data_surrogate['Delay'] - 0.5\n",
    "data_surrogate['Delay']=data_surrogate['Delay'].round(decimals=1)\n",
    "\n",
    "data_surrogate.set_index(['Feature','Freq','Condition','Delay','Subject'], inplace=True)\n",
    "data_surrogate.sort_index(inplace=True)\n",
    "\n",
    "a  = data_surrogate['Data'].apply(lambda x: np.mean(x))\n",
    "data_surrogate['Coherence'] = pd.Series(a, index=data_surrogate.index)\n",
    "a  = data_surrogate['Surrogate'].apply(lambda x: np.mean(x))\n",
    "data_surrogate['SCoherence'] = pd.Series(a, index=data_surrogate.index)\n",
    "data_surrogate.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.dirname(os.path.dirname(os.getcwd())) + '/fieldtrip_eeg_clean\\mat/matlab.mat'\n",
    "connectivity ,ch_names = mne.channels.read_ch_connectivity(fname)\n",
    "\n",
    "connectivity, ch_names = mne.channels.find_ch_connectivity(info.info, ch_type='eeg')\n",
    "\n",
    "\n",
    "plt.imshow(connectivity.toarray(), cmap='gray', origin='lower',\n",
    "           interpolation='nearest')\n",
    "plt.xlabel('{} Magnetometers'.format(len(ch_names)))\n",
    "plt.ylabel('{} Magnetometers'.format(len(ch_names)))\n",
    "plt.title('Between-sensor adjacency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = data_surrogate.reset_index()\n",
    "data = aaa[['Feature','Freq','Condition','Delay','Subject','Data','Coherence']].copy()\n",
    "data.set_index(['Feature','Freq','Condition','Delay','Subject'], inplace=True)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "data_surrogate = aaa[['Feature','Freq','Condition','Delay','Subject','Surrogate','SCoherence']].copy()\n",
    "data_surrogate = data_surrogate.rename(index=str, columns={\"Surrogate\": \"Data\", \"SCoherence\": \"Coherence\"})\n",
    "data_surrogate.set_index(['Feature','Freq','Condition','Delay','Subject'], inplace=True)\n",
    "data_surrogate.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nitime\n",
    "#Import the time-series objects:\n",
    "from nitime.timeseries import TimeSeries\n",
    "#Import the analysis objects:\n",
    "from nitime.analysis import CoherenceAnalyzer\n",
    "\n",
    "data = hyper.get_data()\n",
    "data.shape\n",
    "\n",
    "\n",
    "f_lb = 1\n",
    "f_ub = 3\n",
    "\n",
    "T = ts.TimeSeries(data[0], sampling_rate=400)\n",
    "C = CoherenceAnalyzer(T,method={'NFFT': 256})\n",
    "freq_idx = np.where((C.frequencies > f_lb) * (C.frequencies < f_ub))[0]\n",
    "idx1 = np.repeat(59,59)\n",
    "idx2 = np.arange(0,59)\n",
    "idx3 = np.repeat(60,59)\n",
    "\n",
    "A = []\n",
    "for i in range(0,1):\n",
    "    T = ts.TimeSeries(data[i], sampling_rate=400)\n",
    "    C = CoherenceAnalyzer(T,method={'NFFT': 512})\n",
    "    coh = C.coherence_partial[idx1, idx2, idx3]\n",
    "    coh = np.mean(coh[:, freq_idx], -1)\n",
    "    A.append(coh)\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "C.coherence_partial    \n",
    "C = CoherenceAnalyzer(T,method={'this_method': 'multi_taper_csd','adaptive':'True','low_bias':'True'})\n",
    "C.frequencies\n",
    "C = CoherenceAnalyzer(T,method={'NFFT': 512})\n",
    "C.frequencies[51]\n",
    "C.coherence.shape\n",
    "C.coherence_partial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Import from other libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.mlab import csv2rec\n",
    "\n",
    "import nitime\n",
    "#Import the time-series objects:\n",
    "from nitime.timeseries import TimeSeries\n",
    "#Import the analysis objects:\n",
    "from nitime.analysis import CorrelationAnalyzer, CoherenceAnalyzer\n",
    "#Import utility functions:\n",
    "from nitime.utils import percent_change\n",
    "from nitime.viz import drawmatrix_channels, drawgraph_channels, plot_xcorr\n",
    "\n",
    "#This information (the sampling interval) has to be known in advance:\n",
    "TR = 1.89\n",
    "f_lb = 0.02\n",
    "f_ub = 0.15\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "We use csv2rec to read the data in from file to a recarray:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_path = os.path.join(nitime.__path__[0], 'data')\n",
    "\n",
    "data_rec = csv2rec(os.path.join(data_path, 'fmri_timeseries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract information:\n",
    "roi_names = np.array(data_rec.dtype.names)\n",
    "n_samples = data_rec.shape[0]\n",
    "\n",
    "\n",
    "#Make an empty container for the data\n",
    "data = np.zeros((len(roi_names), n_samples))\n",
    "\n",
    "for n_idx, roi in enumerate(roi_names):\n",
    "    data[n_idx] = data_rec[roi]\n",
    "\n",
    "#Normalize the data:\n",
    "data = percent_change(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TimeSeries(data, sampling_interval=TR)\n",
    "T.metadata['roi'] = roi_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the correlation analyzer\n",
    "C = CorrelationAnalyzer(T)\n",
    "\n",
    "#Display the correlation matrix\n",
    "fig01 = drawmatrix_channels(C.corrcoef, roi_names, size=[10., 10.], color_anchor=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = C.xcorr_norm\n",
    "\n",
    "idx_lcau = np.where(roi_names == 'lcau')[0]\n",
    "idx_rcau = np.where(roi_names == 'rcau')[0]\n",
    "idx_lput = np.where(roi_names == 'lput')[0]\n",
    "idx_rput = np.where(roi_names == 'rput')[0]\n",
    "\n",
    "fig02 = plot_xcorr(xc,\n",
    "                   ((idx_lcau, idx_rcau),\n",
    "                    (idx_lcau, idx_lput)),\n",
    "                   line_labels=['rcau', 'lput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = CoherenceAnalyzer(T)\n",
    "\n",
    "freq_idx = np.where((C.frequencies > f_lb) * (C.frequencies < f_ub))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh = np.mean(C.coherence[:, :, freq_idx], -1)  # Averaging on the last dimension\n",
    "fig03 = drawmatrix_channels(coh, roi_names, size=[10., 10.], color_anchor=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.coherence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.coherence_partial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.hstack([idx_lcau, idx_rcau, idx_lput, idx_rput])\n",
    "idx1 = np.vstack([[idx[i]] * 4 for i in range(4)]).ravel()\n",
    "idx2 = np.hstack(4 * [idx])\n",
    "\n",
    "coh = C.coherence[idx1, idx2].reshape(4, 4, C.frequencies.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx3 = np.hstack(16 * [idx_lcau])\n",
    "coh = C.coherence_partial[idx1, idx2, idx3].reshape(4, 4, C.frequencies.shape[0])\n",
    "coh = np.mean(coh[:, :, freq_idx], -1)\n",
    "coh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh = C.coherence_partial[idx1, idx2, idx3].reshape(4, 4, C.frequencies.shape[0])\n",
    "coh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig05 = drawgraph_channels(coh, roi_names[idx])\n",
    "fig06 = drawmatrix_channels(coh, roi_names[idx], color_anchor=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.datasets import sample\n",
    "from mne import io\n",
    "\n",
    "data_path = sample.data_path()\n",
    "raw_fname = data_path + '/MEG/sample/sample_audvis_filt-0-40_raw.fif'\n",
    "\n",
    "raw = io.RawFIF(raw_fname, preload=True)\n",
    "raw.\n",
    "to_nitime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmin = 0\n",
    "Tmax = 4.51\n",
    "trial_len = 3\n",
    "\n",
    "for s in subject_name:\n",
    "    save_path = data_path + '/analysis/python/data/'+s+'-coh-epo-'+str(Tmin)+'-'+str(Tmax)+'-trialLen-'+str(trial_len)+'.fif'\n",
    "    epochs = mne.read_epochs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "len(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import parallel\n",
    "c = parallel.Client()\n",
    "view = c.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def psum(a):\n",
    "    locsum = np.sum(a)\n",
    "    rcvBuf = np.array(0.0,'d')\n",
    "    MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE],\n",
    "        [rcvBuf, MPI.DOUBLE],\n",
    "        op=MPI.SUM)\n",
    "    return rcvBuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipyparallel as ipp\n",
    "\n",
    "rc = ipp.Client()\n",
    "ar = rc[:].apply_async(os.getpid)\n",
    "pid_map = ar.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import parallel\n",
    "c = parallel.Client()\n",
    "view = c.load_balanced_view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject = dict(eeg=180e-6)\n",
    "events = mne.find_events(raw, stim_channel='Trigger')\n",
    "event_id = {'Speech onset': 5}\n",
    "tmin, tmax = -0.1, 3  # start and end of an epoch in sec.\n",
    "# Set up indices of channels to include in analysis\n",
    "epochs_params = dict(events=events, picks=picks,event_id=event_id, tmin=tmin, tmax=tmax,reject=reject)\n",
    "epochs = mne.Epochs(raw, **epochs_params)\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# let's explore some frequency bands\n",
    "iter_freqs = [\n",
    "    ('Delta', 1, 3),\n",
    "    ('Theta', 4, 7),\n",
    "    ('Alpha', 8, 12),\n",
    "    ('Beta', 13, 25),\n",
    "    ('Gamma', 30, 40)\n",
    "]\n",
    "features = ['envelop','jawaopening','lipaparature','lipProtrusion','TTCD','TMCD','TBCD']\n",
    "condition = ['Hyper','Normal','Hypo']\n",
    "\n",
    "frames = []\n",
    "for f in range(0,len(features)):\n",
    "    for band, fmin, fmax in iter_freqs:\n",
    "        feature = features[f]\n",
    "        hyper, normal, hypo = listen_italian_functions.coherence_freq(epochs,fmin, fmax,feature)\n",
    "        a = band+ ' '+str(fmin)+' - '+str(fmax)+'Hz'\n",
    "        df = pd.DataFrame({'Condition':'Hyper','Freq':a,'Feature':feature,'Data':[hyper.flatten()]})\n",
    "        df = df.append(pd.DataFrame({'Condition':'Normal','Freq':a,'Feature':feature,'Data':[normal.flatten()]}))\n",
    "        df = df.append(pd.DataFrame({'Condition':'Hypo','Freq':a,'Feature':feature,'Data':[hypo.flatten()]}))\n",
    "        frames.append(df)\n",
    "    \n",
    "data=pd.concat((frames),axis=0)\n",
    "data.set_index(['Feature','Freq','Condition'], inplace=True)\n",
    "data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.drop([0], reason='User reason')\n",
    "epochs.drop_bad(reject=dict(grad=2500e-13, mag=4e-12, eog=200e-6), flat=None)\n",
    "print(epochs.drop_log)\n",
    "epochs.plot_drop_log()\n",
    "print('Selection from original events:\\n%s' % epochs.selection)\n",
    "print('Removed events (from numpy setdiff1d):\\n%s'\n",
    "      % (np.setdiff1d(np.arange(len(events)), epochs.selection).tolist(),))\n",
    "print('Removed events (from list comprehension -- should match!):\\n%s'\n",
    "      % ([li for li, log in enumerate(epochs.drop_log) if len(log) > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "evoked = epochs.average()\n",
    "# evoked.plot_joint(times=[0.105, 0.130, 0.180]);\n",
    "data = epochs.get_data()\n",
    "data_mean = np.mean(data, axis=0)[31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = {'Speech_onset': 5, 'Speech_offset': 6}\n",
    "color = { 5: 'blue', 6: 'red'}\n",
    "\n",
    "a=mne.viz.plot_events(events, raw.info['sfreq'], raw.first_samp, color=color,event_id=event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnetometer data as an animation\n",
    "a=hyper.animate_topomap(times=times, frame_rate=10,time_unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject = dict(eeg=180e-6)\n",
    "events = mne.find_events(raw, stim_channel='trial_no')\n",
    "event_id = {'Speech_onset': 105}\n",
    "tmin, tmax = -0.1, 2  # start and end of an epoch in sec.\n",
    "# Set up indices of channels to include in analysis\n",
    "picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=True, eog=False,misc=True)\n",
    "epochs_params = dict(events=events, picks=picks,event_id=event_id, tmin=tmin, tmax=tmax,reject=reject,preload=True)\n",
    "epochs = mne.Epochs(raw, **epochs_params)\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#behaviour\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "raw_fname = data_path + '/analysis/behaviour/data/subject/'+subject_name+'_behaviour.mat'\n",
    "mat = scipy.io.loadmat(raw_fname)\n",
    "condition= mat['behaviour']['condition'][0]\n",
    "response= mat['behaviour']['response'][0]\n",
    "a = np.hstack((condition[0],response[0]))\n",
    "\n",
    "df = pd.DataFrame({'condition':a[:,0],'response':a[:,1]})\n",
    "df.index = df.index + 1\n",
    "\n",
    "hyper  = df.loc[(df['condition'] == 1) & (df['response'] == 0)]\n",
    "normal = df.loc[(df['condition'] == 2) & (df['response'] == 0)]\n",
    "hypo   = df.loc[(df['condition'] == 3) & (df['response'] == 0)]\n",
    "\n",
    "events = mne.find_events(raw, stim_channel='trial_no')\n",
    "hyper = np.intersect1d(events[:,2], hyper.index.values)\n",
    "normal = np.intersect1d(events[:,2], normal.index.values)\n",
    "hypo = np.intersect1d(events[:,2], hypo.index.values)\n",
    "\n",
    "hyper = events[hyper-1]\n",
    "hyper[:,2] = 1\n",
    "normal = events[normal-1]\n",
    "normal[:,2] = 2\n",
    "hypo = events[hypo-1]\n",
    "hypo[:,2] = 3\n",
    "a = np.vstack((hyper,normal,hypo))\n",
    "a = np.sort(a, axis=0)\n",
    "\n",
    "# add this events to raw \n",
    "stim_data = np.zeros((1, len(raw.times)))\n",
    "info = mne.create_info(['con'], raw.info['sfreq'], ['misc'])\n",
    "stim_raw = mne.io.RawArray(stim_data, info)\n",
    "raw.add_channels([stim_raw], force_update_info=True)\n",
    "raw.add_events(a, stim_channel='con')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject = dict(eeg=180e-6)\n",
    "events = mne.find_events(raw, stim_channel='con')\n",
    "event_id = {'hyper': 1,'normal': 2,'hypo': 3}\n",
    "tmin, tmax = -0.5, 2  # start and end of an epoch in sec.\n",
    "# Set up indices of channels to include in analysis\n",
    "picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=True, eog=False,misc=True)\n",
    "epochs_params = dict(events=events, picks=picks,event_id=event_id, tmin=tmin, tmax=tmax,reject=reject,preload=True)\n",
    "epochs = mne.Epochs(raw, **epochs_params)\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and order with spectral reordering\n",
    "# If you don't have scikit-learn installed set order_func to None\n",
    "from sklearn.cluster.spectral import spectral_embedding  # noqa\n",
    "from sklearn.metrics.pairwise import rbf_kernel   # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def order_func(times, data):\n",
    "    this_data = data[:, (times > 0.0) & (times < 0.350)]\n",
    "    this_data /= np.sqrt(np.sum(this_data ** 2, axis=1))[:, np.newaxis]\n",
    "    return np.argsort(spectral_embedding(rbf_kernel(this_data, gamma=1.),\n",
    "                      n_components=1, random_state=0).ravel())\n",
    "\n",
    "good_pick = 10  # channel with a clear evoked response\n",
    "bad_pick = 12  # channel with no evoked response\n",
    "\n",
    "# We'll also plot a sample time onset for each trial\n",
    "plt_times = np.linspace(0, .2, len(epochs))\n",
    "\n",
    "plt.close('all')\n",
    "a=mne.viz.plot_epochs_image(epochs, [good_pick, bad_pick], sigma=.5,order=order_func, vmin=-40, vmax=40,\n",
    "                          overlay_times=plt_times, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
